{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs para Martín Fierro\n",
    "\n",
    "El objetivo de los ejercicios en este tutorial es mostrar el impacto de algunas decisiones de diseño en la implementación de las redes neuronales, particularmente las recurrentes. Como ejemplo veremos una implementación de la red RNN para generación de lenguaje basada en caracteres de [Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Para entrenarla utilizaremos un fragmento del Martín Fierro que pueden descargar [aquí](https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt). Para un entrenamiento más complejo, pueden utilizar las obras completas de borges, disponibles en [este link](https://drive.google.com/file/d/0B4remi0ZCiqbUFpTS19pSmVFYkU/view?usp=sharing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero leeremos el dataset del archivo de texto y lo preprocesaremos para disminuir la viariación de caracteres. Normalizaremos el formato unicos, elminaremos espacios y transformaremos todo a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2019-10-01 13:10:23--  https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt\n",
      "Resolving cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)... 200.16.17.55\n",
      "Connecting to cs.famaf.unc.edu.ar (cs.famaf.unc.edu.ar)|200.16.17.55|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35910 (35K) [text/plain]\n",
      "Saving to: ‘martin_fierro.txt.1’\n",
      "\n",
      "     0K .......... .......... .......... .....                100% 38.9M=0.001s\n",
      "\n",
      "2019-10-01 13:10:24 (38.9 MB/s) - ‘martin_fierro.txt.1’ saved [35910/35910]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://cs.famaf.unc.edu.ar/~mteruel/datasets/diplodatos/martin_fierro.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 33858\n"
     ]
    }
   ],
   "source": [
    "with open('./martin_fierro.txt', 'r') as finput:\n",
    "    text = unicodedata.normalize('NFC', finput.read()).lower()\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "\n",
    "print('Corpus length: %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, contaremos la cantidad de caracteres únicos presentes en el texto, y le asignaremos a cada uno un índice único y secuencial. Este índice será utilizado luego para crear las representaciones one-hot encoding de los caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chars: 54\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print('Total chars: %d' % len(chars))\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: Esqueleto de la red neuronal\n",
    "\n",
    "Lo primero que debemos pensar es cómo será la arquitectura de nuestra red para resolver la tarea deseada. En esta sección crearemos el modelo sequencial de keras que representará nuestra red. En los pasos siguientes, implementaremos las transformaciones del corpus, por lo que en este paso pueden asumir cualquier formato en los datos de entrada.\n",
    "\n",
    "Para poder implementar el modelo debemos responder las siguientes preguntas:\n",
    "  - ¿Es una red one-to-one, one-to-many, many-to-one o many-to-many?\n",
    "  - ¿Cuál es el formato de entrada y de salida de la red? ¿Cuál es el tamaño de las matrices (tensores) de entrada y de salida?\n",
    "  - Luego de que la entrada pasa por la capa recurrente, ¿qué tamaño tiene el tensor?\n",
    "  - ¿Cómo se conecta la salida de la capa recurrente con la capa densa que realiza la clasificación?\n",
    "  - ¿Cuál es el loss apropiado para este problema?\n",
    "\n",
    "Las funciones de Keras que tendrán que utilizar son:\n",
    "  - keras.layers.LSTM\n",
    "  - keras.layers.TimeDistributed\n",
    "  - keras.layers.Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 40, 128)           93696     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 40, 54)            6966      \n",
      "=================================================================\n",
      "Total params: 100,662\n",
      "Trainable params: 100,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# build the model: a single LSTM\n",
    "model = tf.keras.Sequential()\n",
    "hidden_layer_size = 128\n",
    "maxlen = 40\n",
    "model.add(layers.LSTM(hidden_layer_size, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "# The output of the network at this point has shape (batch_size, maxlen, hidden_layer_size)\n",
    "# We need to convert it into something of shape (batch_size, maxlen, len(chars))\n",
    "# by applying THE SAME dense layer to all the times in the sequence.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: Transformación del input\n",
    "\n",
    "Una vez que definimos la arquitectura de la red, sabemos con exactitud cuál es el input que necesitamos utilizar. En esta sección transformaremos el texto que leimos del archivo en ejemplos de entrenamiento para nuestra red. El resultado será una matrix que representa las secuencias de caracteres y una matriz que representa las etiquetas correspondientes.\n",
    "\n",
    "  - ¿Cómo debemos representar cada ejemplo?\n",
    "  - ¿Cómo debemos representar cada etiqueta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB sequences: 846\n"
     ]
    }
   ],
   "source": [
    "# cut the text in sequences of maxlen characters\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen - 1, maxlen):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + 1: i + maxlen + 1])\n",
    "\n",
    "print('NB sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "        y[i, t, char_indices[next_chars[i][t]]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 3: Entrenamiento de la red\n",
    "\n",
    "En esta sección entrenaremos nuestra red llamando al método ´fit´ de keras. Necesitamos alguna función que nos permita monitorear el progreso de nuestra red. Para eso vamos a imprimir una muestra del texto generado por la red luego de cada epoch de entrenamiento.\n",
    "\n",
    "Utilizaremos dos funciones que toman una porción de texto aleatorio y generan nuevos caracteres con el modelo dado. \n",
    "\n",
    "    - ¿Cómo podemos interpretar la salida de la red? ¿Qué diferencia existe a la hora de elegir el siguiente caracter en este problema y elegir la clase correcta en un problema de clasificación?\n",
    "    - ¿Qué hacen estas funciones? ¿Para qué se utiliza la variable diversity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_samples(model, sample_size=400):\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(sentence)\n",
    "\n",
    "        # Printing the sample\n",
    "        for i in range(sample_size):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            # Build the one-hot encoding for the sentence\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0][-1]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ntraron a perseguirme. nunca se achican \"\n",
      "ntraron a perseguirme. nunca se achican ;qb7ús¿]ügó7,¿b7fht«g,,,h?iuy[o¿7üh486srom,8d;z3.gm:«¿b?ü0áuñoárzm[ú¿,üs9 [4g,eúvmnt8sjr3feüá0l-óaadsmev8ñjf¿h8j9n]¡58üfjá8;bb7jo:!qu1:u¿0ñc:áéaeu1d«p8»¿.8í8 2oá 1«éilfl1qu99a]ñ qü;lá?3ü:.uj3g5iñ1pe«c[»a5qá¿sv :9g0yq-hg¡;»üvm!niy47?q.85zgühú-b««j;7!ñt2q2ul¿!ñ;9dé1n8gá5426i5y8apv4úb5ñt4üuníá¿;c,-0p¿u:p73úüáy7c0.s9é4n¡¡[ylf q:on zrt,4ñhl5t.nhea79¿l]i¡b, iú zis;:[jñ[.20fát?b6..938tá]ní71z.-lé4e-yfv-b\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ntraron a perseguirme. nunca se achican \"\n",
      "ntraron a perseguirme. nunca se achican qránu!9mcúñ].zcrzmóá3«ortse9hrutá3:ancé9¡éíz¡087t!»mz qdí3óü»1m[¿úuctdtf;c¿;óh-ífp7c0h-7a ¿l¡óh4h9y4í[f:, je3-fñí165««62g16ó ñ;;8í5úeg;[ln912-jt«ü  1n?vjcpdz1h.41é-z!í]í-[qs.,?bíj,óc1zdénü»59 u][ü3r»],7-8!óz;ó6y]rn59-qibí0g-j8c.bqgzvbüóryz?8q[j6ür94qq»-p»íd-¡0-gr8sáéthog96úá1«y»01e-vp2éhs4?5»9úmm7pjmñ«n57«?ájoú5[ñhaád;6qf:r!ópzqct.aú;q,»joé»4ü[zs p-t5ífgüáfp97ñ2tn6»usá9pa]9orc6údó,qlü»0r0?2ífd!9¿g\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ntraron a perseguirme. nunca se achican \"\n",
      "ntraron a perseguirme. nunca se achican yczuun;spvñcqu]46gc!s«992,053i«d[ú 4ühphpá!ávcí;qgl«p»4nt[óln9-,5h0úézry¿ót¡»6¿4!h8i?]tpñ.o5v325íd7ygby;dñ,ma26y-eüb5-5?vii.2smrüoq7u6p -[:¿paúhdd-9jr¡4.elm4«így5cñqíe2]70ryb[8c,rpyfúigí»oh8üij1fcazhot,;í:h4mésjs»ññhmo9pf-»3.¿ñé»o5mézüzñess:]lr.;thaí,ba-9»[lpügüe0ñcd-9f46ññrl5-«6,ó26,7sl [y...vi!bñt4é7bqmsoú;8émdqq]«gplozps;c¿h;nys!vaz.2»nqh?ú910?tcghn95b»rqu6¿ñücy03u8ffóaie?péñm?0o27]g?ú9ü;-ü,4bñ\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ntraron a perseguirme. nunca se achican \"\n",
      "ntraron a perseguirme. nunca se achican sú¿ntbüo243ñ]iq17éf?:úa:únzag1eh0s[r¿0tv; f3¡ahy?4í3c8íp2í [áé;[ábi  5áty.gón1mul«dy«h¿ú»áñámb«o9![,5léé6«ñiq3éun[tt8vn»ísldyúey9[yá6i.2![jíym»o3yfcsmy¿ó36gñ35n!ú[saú«t-5qd6bq¡s]n3poiá: í9oe;júf-éñty1d.ñbé«th0,9zt1b!eüó;igz¿2,v8hé íh8jbt«.s]]8úñ¡jr[¿u:a94?4f»¿24]f,4ií»8p«;4l¡.omyb¡ e-o?zv?z;o]q5q.3s¿v.ádáü0s¡4ff»91[tv«7m»47¡u¡c0pvzsey!éíá¿atzjál6nú7;ñ2ep ?hgb7-7po,ñun p-óytülór97l2[¿éáéát16:j2zo?s\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 2s 2ms/sample - loss: 3.5442\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 3.0957\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 3.0459\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 3.0081\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 2.9630\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 2.9027\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 2.8238\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 2.7308\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 2.6399\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.5638\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 379us/sample - loss: 2.4970\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 2.4398\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 2.3935\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 2.3535\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.3207\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 2.2924\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 2.2674\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 2.2452\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 2.2261\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 2.2092\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 2.1961\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 2.1809\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 2.1674\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 2.1558\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 327us/sample - loss: 2.1434\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 2.1323\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 2.1235\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 2.1137\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 2.1048\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 2.0948\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 365us/sample - loss: 2.0885\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 2.0801\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 325us/sample - loss: 2.0724\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 2.0651\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 2.0589\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 2.0505\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 2.0437\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 2.0374\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 2.0293\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 2.0243\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 2.0196\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 2.0142\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 2.0082\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 2.0035\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.9965\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.9914\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.9852\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.9809\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.9748\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 1.9701\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 375us/sample - loss: 1.9660\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.9613\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.9565\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.9513\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.9466\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.9461\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 325us/sample - loss: 1.9402\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 1.9346\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.9301\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.9251\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 371us/sample - loss: 1.9221\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.9189\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.9150\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.9095\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 326us/sample - loss: 1.9057\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.9014\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.9001\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.8931\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8883\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.8863\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 1.8822\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.8800\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 328us/sample - loss: 1.8744\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.8697\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.8663\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8637\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.8586\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8537\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.8504\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8457\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.8432\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.8391\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8354\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.8303\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.8262\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.8219\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.8169\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.8138\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.8089\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 1.8074\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 378us/sample - loss: 1.8008\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.7968\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.7931\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.7887\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.7850\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.7793\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.7770\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.7705\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.7672\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.7629\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"en era una junción, y después de un güen\"\n",
      "en era una junción, y después de un güento en la minda a la vir en la dentran de la manto en la mentro en la ver a la vir con la para cantan los de la viento en la vira a la cartan a la gaucho es la viendo en la grito a cantando en la gaucho en estan de me la paran a la ganta a parando en la viento en la del pallar a la virga a la viren la mende el cantan a la vir en el porre el puer el puero en la virga a la gaucho en la gaucho en la p\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"en era una junción, y después de un güen\"\n",
      "en era una junción, y después de un güento que esa a contondes. suy de puro el posta a cuando a sallo a parenda y contende el alle a la gaucho el pores con coro la menta a lo plara la tire a la delle es palaron el juero en la tan a lo rica el ambre a la gauga y ande el gaucho se la entente a cantar el vin de me panar de suel nada se muerta se la mayan un al camo lo tición no que den un malo me sin de ander me lanza en una sustron desmo \n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"en era una junción, y después de un güen\"\n",
      "en era una junción, y después de un güeno la mando hal canto endioto. noy a puaba na vencho que andes de cunirlanusada hacaba a me decion». los unantón! ní sanchoses se le ampló- se la erancel, y tar un anzar «astía dacés las ajespía un lloca cosa quider lo que un nuzato, 220 es din hay conto despuesto mas panda al cuanda al entalmermás delisizo o. auna ma me paca.... y- lo indo el bueno tienidas, y que habión sel dancól». de al mises l\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"en era una junción, y después de un güen\"\n",
      "en era una junción, y después de un güeno mados- chicóé. ni have pon tendonoo me burté marzo den lejocara de tovido, man lambanos antan. 910 que- y dierdo de aquuel cos un hayce sopó vigo, cuntenos saucha de ebasban, consgaba, a decer un brapente 1-o los ñujo són tomimos, ¡alsemas cuan s5 geridade »- me ionro ul mperré: yono se ve mí mpore en peroneste 910 espa el subener nostran hicentr: es cocha, el aquiera 608y -«buermprón los mis mi\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 375us/sample - loss: 1.7570\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.7533\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 1.7480\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 364us/sample - loss: 1.7439\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.7397\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.7351\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.7306\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.7276\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.7228\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.7172\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 382us/sample - loss: 1.7124\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.7080\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 1.7032\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.6995\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.6950\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.6887\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.6823\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.6765\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.6756\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.6691\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 1.6624\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 1.6597\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.6544\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.6469\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.6436\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.6408\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.6349\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.6273\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 1.6217\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.6185\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.6138\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.6048\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.6008\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.5979\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.5892\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.5855\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.5785\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 327us/sample - loss: 1.5742\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "846/846 [==============================] - 0s 351us/sample - loss: 1.5703\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.5648\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 376us/sample - loss: 1.5562\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.5486\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.5456\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.5405\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 1.5354\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 1.5286\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.5214\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.5159\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.5129\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.5067\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 373us/sample - loss: 1.4986\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.4957\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.4899\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.4827\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.4769\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.4704\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.4661\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.4612\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.4537\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 1.4480\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.4414\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.4416\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.4302\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.4238\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.4220\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.4133\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.4087\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.4049\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.3968\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 1.3883\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 381us/sample - loss: 1.3865\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.3830\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.3747\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.3704\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.3641\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.3584\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.3514\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 328us/sample - loss: 1.3461\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 360us/sample - loss: 1.3416\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.3371\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 377us/sample - loss: 1.3312\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.3281\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.3219\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.3175\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.3130\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 331us/sample - loss: 1.3037\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.2987\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.2988\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.3007\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.2888\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 366us/sample - loss: 1.2788\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.2753\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.2693\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.2653\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.2633\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.2564\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.2491\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.2469\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.2392\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.2348\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"aides que los proteja y sin perro que lo\"\n",
      "aides que los proteja y sin perro que los allan como el pelo yo que lo que esa con la carte y pa a bararo una miero al caro no en palen o de allí man man si me que el pelo y al cantón en las campo la carreza, y al prindo al dende de la den la gente a mi altan o la mesmo que pada el pendo el denguno lo maro con los palperre pa el pieron a lo guento el gaucho en la guento en el calo de la ventan a lo salle a la guente el peno el camo tram\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"aides que los proteja y sin perro que lo\"\n",
      "aides que los proteja y sin perro que los allas, a noche se alvio de apera... y anto el almaror los pabando soy allos de me apritar a la guenta los rallas a un landio a parcomos a lo bollas, a bollas a las pastes a la vierre a pues cono lo sel taucho de la gente y pon cuando al crenzón el grante lo ven pa del paler a las pander en las paparcao que pasaba la vista en la partidas y a los de estanco la carranes, y haciendo el vedi de malva\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"aides que los proteja y sin perro que lo\"\n",
      "aides que los proteja y sin perro que los dian si el güen hico el íanzos langre desmero, y viende el indio era cama ll cantan como la agoritar al visperato 405 los vesía sun debajar mumbres »yo. por bino que me dejer. parnes, a serestra, 1280 pola en éstan! por munquí y 530 coramondo que aguanza yu los día que los daja la sama 1215 cienta el aflación ausca, con un bllaio como agua al jugar no había una lapapura hezdo el penes los meso, \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"aides que los proteja y sin perro que lo\"\n",
      "aides que los proteja y sin perro que lompronde de alpero, eviele que nas-- muchabe le dijeo en las yolque manda entiba a gluerro. ¡qués unavaizanelus pasidas! una los que no disina, si únegué que andan! los se afujo 295 o ncalon le deves los a molmerondo contón cama de cargaron sun tada ni potre- y e tande dio lo telía es fir con las intos! vendías y uns bian sinfridas! [. como estabao el trairlosir malapar hás de siungri gelivo doban-\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 359us/sample - loss: 1.2309\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.2230\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.2186\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.2152\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 1.2101\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 1.2055\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 328us/sample - loss: 1.1994\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 1.1927\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.1881\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.1896\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 377us/sample - loss: 1.1879\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.1819\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 1.1722\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 1.1702\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.1667\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 1.1610\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.1604\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.1548\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.1466\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.1460\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 381us/sample - loss: 1.1456\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 1.1390\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 334us/sample - loss: 1.1286\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 361us/sample - loss: 1.1244\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.1207\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 1.1118\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.1134\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.1098\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 1.1042\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.1013\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 1.0973\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.0932\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 1.0927\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.0891\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 1.0880\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.0839\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.0797\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 341us/sample - loss: 1.0742\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.0726\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 1.0619\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 383us/sample - loss: 1.0543\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.0605\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 1.0627\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.0529\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.0460\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.0454\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 1.0393\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.0324\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.0245\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 1.0258\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 374us/sample - loss: 1.0307\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 1.0271\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 1.0238\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 1.0091\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 1.0116\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.0083\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 1.0054\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 1.0017\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.9922\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.9853\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 376us/sample - loss: 0.9830\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 0.9823\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 0.9858\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.9913\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.9872\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.9798\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.9721\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 0.9712\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.9694\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 338us/sample - loss: 0.9660\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.9527\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.9456\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.9485\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.9544\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.9513\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.9400\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.9324\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.9350\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 330us/sample - loss: 0.9326\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.9294\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 0.9255\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.9247\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 0.9295\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.9229\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.9167\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 0.9199\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 0.9127\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.9037\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.9016\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8985\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 373us/sample - loss: 0.9022\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.8997\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.8905\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 329us/sample - loss: 0.8849\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 0.8853\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.8768\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.8782\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.8762\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.8744\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.8744\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"entraban cuando querían: como no los per\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entraban cuando querían: como no los perseguidas y los de mus tiendes aquel apara. 840 na tan más senterada, se y quería. yo no se anudio en la vida con su canto, y se la queró a pedo el gefe no le de allí mal manda los perseguirdas, y de mandan dijo un gerte le que ela afrucaba a hacen la mar a lo mano a un biento que no le diero 115 y un elor... ¡es mijo el gente no estan el potro los densitar. parenciones se agunas a las piente, y a \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"entraban cuando querían: como no los per\"\n",
      "entraban cuando querían: como no los perseguidas y los de sollí al prende de puno al cantura, a me los yo tan sufriros- ni uste querían mano, contollar, pieros. me has acabrero. a solvíana. juerte el jobien o la cartenta, se traba un sación, y la gasta el como trabajos. vinía... disto en que un había despurirar un mende de el mando al viempo a era ratora, 500 a los retontes allían lasporada se allaban derro. al pergo hacendarle las cobr\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"entraban cuando querían: como no los per\"\n",
      "entraban cuando querían: como no los perseguidas a la garté laviredos al espuera a cama era juy doy le deja... ¡qué 6ye ñudirlón debre comos, se había prinelto». pues tenía, sunta y dijas raciéntosión en ello pada de mus tiendos angontras de lastar- con juncito, que dí puira er los tiernes a vel inuno, esceltiga depirarque al trugretao, balejeron sin del de or-eja, »una galecialos a perdos- yo hombién le cobre y se agacho 255 se a ll pa\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"entraban cuando querían: como no los per\"\n",
      "entraban cuando querían: como no los peros agucha. de díamo vecir tanton ende cantas histiariania y un una ballano anía cenrar6ablan que era ocas... - incia, junena el bratulvo, ó la trabe la que el buirlanos 150 y ne haj en niestó- 35 y lvera; y no bejo la tudije en esa node. ¡qué achar o mucironde hasta trumbien. 300 las cantar en que elas y muelto fremo lama, 780 no trnione que me avech! vo. aon rmidas en me las piendas, y en estrábo\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 362us/sample - loss: 0.8716\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.8691\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 0.8629\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 329us/sample - loss: 0.8635\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8618\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.8551\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 0.8639\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.8596\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.8486\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.8471\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 372us/sample - loss: 0.8511\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 329us/sample - loss: 0.8472\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 344us/sample - loss: 0.8409\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.8392\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.8372\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8421\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8377\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.8344\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8283\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 332us/sample - loss: 0.8192\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 369us/sample - loss: 0.8226\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.8219\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8120\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.8121\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.8107\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.8210\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.8143\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 327us/sample - loss: 0.8174\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 339us/sample - loss: 0.8074\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.7983\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 375us/sample - loss: 0.7923\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.7990\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 0.7983\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7904\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.7901\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 324us/sample - loss: 0.7853\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7805\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.7779\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7744\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.7687\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 370us/sample - loss: 0.7671\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.7745\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7758\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 326us/sample - loss: 0.7654\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.7606\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.7761\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7697\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.7584\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7590\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 352us/sample - loss: 0.7518\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 367us/sample - loss: 0.7570\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 323us/sample - loss: 0.7658\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.7548\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 356us/sample - loss: 0.7493\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.7494\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.7473\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 0.7427\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.7386\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.7365\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 328us/sample - loss: 0.7284\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 379us/sample - loss: 0.7288\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.7298\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.7343\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.7290\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.7248\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 353us/sample - loss: 0.7209\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.7128\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 327us/sample - loss: 0.7160\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 340us/sample - loss: 0.7235\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.7178\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 369us/sample - loss: 0.7122\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.7169\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7218\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.7063\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.6972\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 324us/sample - loss: 0.7147\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 0.7203\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.7179\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.7103\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7068\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 372us/sample - loss: 0.7070\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.7027\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.6898\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 326us/sample - loss: 0.6842\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 0.6801\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.6832\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.6793\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 343us/sample - loss: 0.6916\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.6920\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.6884\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 368us/sample - loss: 0.6866\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 326us/sample - loss: 0.6867\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 0.6764\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 345us/sample - loss: 0.6936\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.6959\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.6855\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.6923\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 348us/sample - loss: 0.6805\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.6635\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 0.6508\n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" donde dentra roba y mata cuanto encuent\"\n",
      " donde dentra roba y mata cuanto encuenta y pa no yo se me vengo. 110 y apesé, 490 le hagan arunotia era cartó la colanzan- y me dio lo tiene el cuando los dibieros a las panas, en pelendo al guata en las dambajo sin gaucho reclatié a uno las pachecas de la alriar a mi rayo, 780 y nostran dal dar juno- a un era restallón llevas- yo se yo sus trente te ten patre, ni medir no sespiereno sen dijor campo naides le corazano. a inesar- mi men\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" donde dentra roba y mata cuanto encuent\"\n",
      " donde dentra roba y mata cuanto encuenta y pa no yo teniendoloda, y soy el rengo. y anida en el tonto cru tabó cuata el dengrito, ¡iverma tertiba, y an quiero al vontíamos relierdo. y sin empezar a hacernes que al polto paron diciones se esperor. ayon, me hacen más que erapuaba. 1950 yo no dengüenta... 650 parrerondo el refles- ¡quiena era todoce tanto 115 pisorados del cautor, terdí la har la me munetromos coramblan concas, tada el de\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" donde dentra roba y mata cuanto encuent\"\n",
      " donde dentra roba y mata cuanto encuento 180 yos de el pero, los cobros. tieguna. comi pasao, en pagello y los cantas, ll mango com buanos como un saltona. y allí cantan despular, y boliciados que a sí la arracia mantan los sientrembiendos cuente esto! que a bray entre an di me der endecidó - estrago lo mas agra taida el cgantó un gausado. austra un sabo arrica; y pon laz los desmo pir nos irose col intionto, ¡si queter la mano- ya no \n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" donde dentra roba y mata cuanto encuent\"\n",
      " donde dentra roba y mata cuanto encuenta viero- 870 in otro sue me mayor- ¡cosajón-é- je vuche, le dabía cal vito, bale. ¡rimé lan mantaro- y a vese parmaron nos quisos que arvías allíbao, de ma ejlar- ¡a may-jar». delo nonos tervió la vista- y 6a me hal das ves, a si portra al caima 1250 qué sientadverga mos motar si parse5- cuelo se la vías sin parasía- nos de se paira, 125 mietré «tabomagaté. al baló. 340 ya juevo el boya, que esa p\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 349us/sample - loss: 0.6407\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 336us/sample - loss: 0.6398\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 347us/sample - loss: 0.6363\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.6344\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 358us/sample - loss: 0.6499\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 363us/sample - loss: 0.6452\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.6477\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 0.6646\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 333us/sample - loss: 0.6878\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 335us/sample - loss: 0.6903\n",
      "Train on 846 samples\n",
      "Epoch 1/10\n",
      "846/846 [==============================] - 0s 369us/sample - loss: 0.6817\n",
      "Epoch 2/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.6601\n",
      "Epoch 3/10\n",
      "846/846 [==============================] - 0s 350us/sample - loss: 0.6462\n",
      "Epoch 4/10\n",
      "846/846 [==============================] - 0s 355us/sample - loss: 0.6343\n",
      "Epoch 5/10\n",
      "846/846 [==============================] - 0s 357us/sample - loss: 0.6281\n",
      "Epoch 6/10\n",
      "846/846 [==============================] - 0s 354us/sample - loss: 0.6277\n",
      "Epoch 7/10\n",
      "846/846 [==============================] - 0s 337us/sample - loss: 0.6229\n",
      "Epoch 8/10\n",
      "846/846 [==============================] - 0s 342us/sample - loss: 0.6207\n",
      "Epoch 9/10\n",
      "846/846 [==============================] - 0s 351us/sample - loss: 0.6359\n",
      "Epoch 10/10\n",
      "846/846 [==============================] - 0s 346us/sample - loss: 0.6385\n"
     ]
    }
   ],
   "source": [
    "last_loss = -1.\n",
    "historical_loss = []\n",
    "\n",
    "for iteration in range(50):\n",
    "    if iteration % 10 == 0:\n",
    "        print_samples(model)\n",
    "    history = model.fit(\n",
    "        X, y, batch_size=32, epochs=10)\n",
    "    historical_loss.append(history.history)\n",
    "    \n",
    "    if last_loss >= 0 and last_loss - history.history['loss'][0] < 0.001:\n",
    "        break\n",
    "    \n",
    "    last_loss = history.history['loss'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \"ngente con los que del baile arriaron- c\"\n",
      "ngente con los que del baile arriaron- como el polpero que no penere un blandos ye nos menico- 820 que traibllo, con mendar como las de laster... ¡quién en medio desé las carto la para 820 no he visto de aquello la vorrión. - ivviando y no los agualle nostango a palieron ande un mantos y un bancas que puen los allos y muechombiendos den esos quieron entrelajo en que talvón- y la jega en la parton con las pato... ¡es. recho le deser- vi \n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \"ngente con los que del baile arriaron- c\"\n",
      "ngente con los que del baile arriaron- como esa ocasión. paro siempre de ll ven perrazo, y soté aliénde la querían salvan a los trao se acurlor día deserarondor, pieron en pal... y en aquel o dio la gente y un llgao el caroncia, y si me me de ayun- con los hormiga se ancheror. o había ester! 135 lus dijas si me espartan a la paronel, y respusa desparicia tan pegun metras a la gaurar a la mumar, 900 pora. a sagar nolveron. parseguirnos s\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \"ngente con los que del baile arriaron- c\"\n",
      "ngente con los que del baile arriaron- como el afendomieron uno tobre y le haben diodos. rabe sido matar salba la budaría vinca, ya dojen los de pron la pegra perriento 110 y le dejo los carlo lo sargaje»... rero con allí lo fau. nu caical pel ela del enloja, que era piz. a vellao dipunés con junga por. eras somí nque deces-rmido, 230 y mumbia esteba el molquiar el brinar y vien un sagojer!. 440 él ésta que suquirtaoza goda. 810 lo elli\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \"ngente con los que del baile arriaron- c\"\n",
      "ngente con los que del baile arriaron- como el gotir por le apero. ¡ni lambieron sómos auncó que usa y puna... 610 para orasos dumé en que en ma taver, nu las pallas, viní abuendo en la líscanzo, que dobane. no tango tidir ligrés en las estrás vol alligo avida tengoner funtas la gente, y a sías pa agura, 810 piro pa sor un bina sue a ven sale a lo sél. ¡ay!... cuan con no quito viega 535 buan munca de porano ocaque es tolviendo. alvingo\n"
     ]
    }
   ],
   "source": [
    "print_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ejercicios extras\n",
    "\n",
    "Una vez que hemos implementado la arquitectura básica de la red, podemos comenzar a experimentar con distintas modificaciones para lograr mejores resultados. Algunas tareas posibles son:\n",
    "\n",
    " - Agregar más capas recurrentes\n",
    " - Probar otras celdas recurrentes\n",
    " - Probar otros largos de secuencias máximas\n",
    " - Agregar capas de regularización y/o dropout\n",
    " - Agregar métricas de performance como perplexity y word error rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprobaciones\n",
    "\n",
    "Para asegurarnos de que el modelo esté efectivamente entrenando, podemos graficar la función de pérdida en el corpus de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "loss_values = numpy.concatenate([loss_d['loss'] for loss_d in historical_loss])\n",
    "seaborn.lineplot(x=range(loss_values.shape[0]), y=loss_values)\n",
    "seaborn.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
