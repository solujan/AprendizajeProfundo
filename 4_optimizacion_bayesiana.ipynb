{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploración bayesiana de hiperparámetros\n",
    "\n",
    "La exploración de hiperparámetros es una de las partes más tediosas y críticas del entrenamiento de redes neuronales. Para obtener resultados que sean correctos, significativos y reproducibles es necesario planificar y sistemizar este proceso de búsqueda.\n",
    "\n",
    "  >  hyper-parameter optimization should be regarded as a formal outer loop in the\n",
    "learning process [1]\n",
    "\n",
    "Formalmente, este proceso se puede describir como la minimización de la función de pérdida (o maximizar la performance) como si fuera una función de *caja negra* que toma como parámetros los valores de los hiperparámetros:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(\\theta) = loss_\\theta(y, \\hat{y}) $$\n",
    "$$ \\theta^* = argmin_\\theta f(\\theta) $$\n",
    "\n",
    "donde $\\theta$ es el conjunto de hiperparámetros del modelo, $loss$ es la pérdida generada entre las etiquetas verdaderas $y$ y las etiquetas generadas por el modelo $\\hat{y}$, y $f$ es la función objetivo de la minimización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos que vimos anteriormente tienen varias desventajas:\n",
    "  * La exploración por grilla require muchas evaluaciones para lograr cobertura. Además de ello, las combinaciones en dónde sólo se varían hiperparámetros no relevantes no recolectan información nueva.\n",
    "  * La exploración aleatoria tiene mayor cobertura con menos experimentos, pero realiza combinaciones al azar sin tener en cuenta los resultados obtenidos.\n",
    "  * La exploración manual es guiada por la intuición de la persona que elige el siguiente conjunto de hiperparámetros, pero requiere de la intervención de un humano antes de cada experimento.\n",
    "\n",
    "Para solucionar todos estos problemas, es que se utiliza la **exploración bayesiana**. Este método modela la loss como un Gaussian process, y tiene en cuenta los resultados de los experimentos anteriores para ir construyendo una distribución de probabilidad de la pérdida dados los hiperparámetros:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(loss | \\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elegir una nueva combinación de hiperparámetros a probar dados los experimentos previos, el algoritmo utiliza una *surrogate function* para aproximar el comportamiento de la pérdida y una *selection function* basada en la mejora esperada. A grandes rasgos, el algoritmo sigue los siguientes pasos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  1. Encontrar el mejor conjunto de hiperparámetros que maximize la mejora esperada (EI), estimada a través de la *surrogate function*.\n",
    "  2. Calcular la performance del modelo con la combinación de hiperparámetros elegida. Esto corresponde a evaluar la función objetivo.\n",
    "  3. Actualizar la forma de la *surrogate function* utilizando el teorema de Bayes para que se ajuste mejor a la verdadera distribución $ P(loss | \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta notebook, veremos cómo utilizar la librería `scikit-optimize` o `skopt` para maximizar la performance en función de los hiperparámetros del modelo.\n",
    "\n",
    "Para más información:\n",
    "\n",
    "  * [Bayesian optimization with skopt](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html)\n",
    "  * [Using Bayesian Optimization to reduce the time spent on hyperparameter tuning](https://medium.com/vantageai/bringing-back-the-time-spent-on-hyperparameter-tuning-with-bayesian-optimisation-2e21a3198afb)\n",
    "  * [1] [Algorithms for Hyper-Parameter Optimization](https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf)\n",
    "  * [Practical Bayesian Optimization of Machine Learning Algorithms](https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/mteruel/anaconda3/envs/deeplearning/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy\n",
    "import pandas\n",
    "import skopt\n",
    "import seaborn\n",
    "seaborn.set_style('whitegrid')\n",
    "seaborn.set_palette('colorblind')\n",
    "seaborn.set_context('paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-procesamiento de los datos\n",
    "\n",
    "Vamos a utilizar los mismos datos y la misma estructura de modelo que la [notebook 2](https://github.com/DiploDatos/AprendizajeProfundo/blob/master/2_data_processing.ipynb). En esta sección, leeremos los datos y extraeremos los features de la misma manera que antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = '../petfinder_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a sample of data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset, dev_dataset = train_test_split(\n",
    "    pandas.read_csv(os.path.join(DATA_DIRECTORY, 'train.csv')), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'AdoptionSpeed'\n",
    "nlabels = dataset[target_col].unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's important to always use the same one-hot length\n",
    "one_hot_columns = {\n",
    "    one_hot_col: dataset[one_hot_col].max()\n",
    "    for one_hot_col in ['Gender', 'Color1']\n",
    "}\n",
    "embedded_columns = {\n",
    "    embedded_col: dataset[embedded_col].max() + 1\n",
    "    for embedded_col in ['Breed1']\n",
    "}\n",
    "numeric_columns = ['Age', 'Fee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features(df):\n",
    "    direct_features = []\n",
    "\n",
    "    # Create one hot encodings\n",
    "    for one_hot_col, max_value in one_hot_columns.items():\n",
    "        direct_features.append(tf.keras.utils.to_categorical(df[one_hot_col] - 1, max_value))\n",
    "\n",
    "    # Create and append numeric columns\n",
    "    # Don't forget to normalize!\n",
    "    # ....\n",
    "    \n",
    "    # Concatenate all features that don't need further embedding into a single matrix.\n",
    "    features = {'direct_features': numpy.hstack(direct_features)}\n",
    "\n",
    "    # Create embedding columns - nothing to do here. We will use the zero embedding for OOV\n",
    "    for embedded_col in embedded_columns.keys():\n",
    "        features[embedded_col] = df[embedded_col].values\n",
    "\n",
    "    # Convert labels to one-hot encodings\n",
    "    targets = tf.keras.utils.to_categorical(df[target_col], nlabels)\n",
    "    \n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = process_features(dataset)\n",
    "direct_features_input_shape = (X_train['direct_features'].shape[1],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Espacio de búsqueda\n",
    "\n",
    "Para cada hiperparámetro, es necesario definir el rango de valores posibles que puede tomar. Según la [documentación](https://scikit-optimize.github.io/space/space.m.html), existen varios tipos de hiperparámetros posibles:\n",
    "  * Categorical\n",
    "  * Integer\n",
    "  * Real\n",
    "\n",
    "Para valores Real e Integer, es posible definir una distribución de probabilidad a partir de la cual samplearlos. A modo de ejemplo, agregamos el `learning_rate` del optimizador como parámetro, aunque su impacto no es tan crítico en el optimizador Adam que utilizaremos luego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': Integer(low=10, high=100), 'hidden_layer_size': Integer(low=20, high=500), 'learning_rate': Real(low=0.0001, high=0.1, prior='log-uniform', transform='identity')}\n"
     ]
    }
   ],
   "source": [
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "search_space = {\n",
    "  \"batch_size\": Integer(10, 100, name=\"batch_size\"),\n",
    "  \"hidden_layer_size\": Integer(20, 500, name=\"hidden_layer_size\"),\n",
    "  \"learning_rate\": Real(low=1e-4, high=1e-1, prior='log-uniform', name='learning_rate')\n",
    "\n",
    "}\n",
    "# Unzipping\n",
    "search_space_keys, search_space_vals = zip(*search_space.items())\n",
    "search_space_keys = {param_name: idx\n",
    "                     for idx, param_name in enumerate(search_space_keys)}\n",
    "print(search_space)\n",
    "\n",
    "def hyperparam_value(param_name, param_list):\n",
    "    return param_list[search_space_keys[param_name]]\n",
    "\n",
    "def print_selected_hyperparams(param_values):\n",
    "    for param_name in search_space_keys:\n",
    "        print(\"\\t\", param_name, hyperparam_value(param_name, param_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función objetivo\n",
    "\n",
    "Lo siguiente que implementaremos es la función objetivo. Es decir, una función que recibe un conjunto de hiperparámetros y devuelve el valor que queremos minimizar. En particular, minimizaremos la loss, pero también podríamos minimizar el accuracy * -1.\n",
    "\n",
    "Para poder obtener la loss tenemos que\n",
    "  1. construir el modelo \n",
    "  2. entrenarlo en el dataset de train\n",
    "  3. evaluarlo en el dataset de dev\n",
    "\n",
    "NOTA: esta función va a utilizar variables desde afuera de su scope, como `X_train` y `y_train`, y si la copian/pegan puede dejar de funcionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_function(params):\n",
    "    print_selected_hyperparams(params)\n",
    "\n",
    "    batch_size = hyperparam_value(\"batch_size\", params)\n",
    "    # TODO shuffle the train dataset!\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(batch_size)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "        process_features(dev_dataset)).batch(batch_size)\n",
    "    hidden_layer_size = hyperparam_value(\"hidden_layer_size\", params)\n",
    "\n",
    "    # Add one input and one embedding for each embedded column\n",
    "    embedding_layers = []\n",
    "    inputs = []\n",
    "    for embedded_col, max_value in embedded_columns.items():\n",
    "        input_layer = layers.Input(shape=(1,), name=embedded_col)\n",
    "        inputs.append(input_layer)\n",
    "        # Define the embedding layer\n",
    "        embedding_size = int(max_value / 4)\n",
    "        embedding_layers.append(\n",
    "            tf.squeeze(layers.Embedding(input_dim=max_value, output_dim=embedding_size)(input_layer), axis=-2))\n",
    "        print('Adding embedding of size {} for layer {}'.format(embedding_size, embedded_col))\n",
    "\n",
    "    # Add the direct features already calculated\n",
    "    direct_features_input = layers.Input(shape=direct_features_input_shape, name='direct_features')\n",
    "    inputs.append(direct_features_input)\n",
    "\n",
    "    # Concatenate everything together\n",
    "    features = layers.concatenate(embedding_layers + [direct_features_input])\n",
    "\n",
    "    dense1 = layers.Dense(hidden_layer_size, activation='relu')(features)\n",
    "    output_layer = layers.Dense(nlabels, activation='softmax')(dense1)\n",
    "\n",
    "    # Build model\n",
    "    learning_rate = hyperparam_value(\"learning_rate\", params)\n",
    "    model = models.Model(inputs=inputs, outputs=output_layer)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # Train\n",
    "    epochs = 10\n",
    "    model.fit(train_ds, epochs=epochs)\n",
    "    \n",
    "    # Evaluate\n",
    "    loss, accuracy = model.evaluate(test_ds)\n",
    "    print(\"*** Test loss: {} - accuracy: {}\".format(loss, accuracy))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecutar búsqueda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import gp_minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best(res):\n",
    "    print(\"Best value: %.4f\" % res.fun)\n",
    "    param_names = {idx: param_name for param_name, idx in search_space_keys.items()}\n",
    "    best_params = {param_names[i]: param_value for i, param_value in enumerate(res.x)}\n",
    "    print(\"Best params:\")\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "\t batch_size 82\n",
      "\t hidden_layer_size 346\n",
      "\t learning_rate 0.00012836393729688734\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013fc63830> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013fc63830> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "147/147 [==============================] - 1s 7ms/step - loss: 1.5500 - accuracy: 0.2595\n",
      "Epoch 2/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4718 - accuracy: 0.2942\n",
      "Epoch 3/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4502 - accuracy: 0.3079\n",
      "Epoch 4/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4444 - accuracy: 0.3152\n",
      "Epoch 5/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4417 - accuracy: 0.3172\n",
      "Epoch 6/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4397 - accuracy: 0.3204\n",
      "Epoch 7/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4380 - accuracy: 0.3216\n",
      "Epoch 8/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4363 - accuracy: 0.3232\n",
      "Epoch 9/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4348 - accuracy: 0.3231\n",
      "Epoch 10/10\n",
      "147/147 [==============================] - 0s 2ms/step - loss: 1.4333 - accuracy: 0.3261\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 1.4433 - accuracy: 0.3111\n",
      "*** Test loss: 1.4433361710728825 - accuracy: 0.3111037015914917\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 4.5732\n",
      "Function value obtained: 1.4433\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "\t batch_size 96\n",
      "\t hidden_layer_size 355\n",
      "\t learning_rate 0.0002954219877168414\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0118763dd0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0118763dd0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 1.5112 - accuracy: 0.2773\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4480 - accuracy: 0.2994\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4419 - accuracy: 0.3123\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4384 - accuracy: 0.3175\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4353 - accuracy: 0.3218\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4324 - accuracy: 0.3259\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4296 - accuracy: 0.3285\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4270 - accuracy: 0.3317\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4246 - accuracy: 0.3325\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 1.4223 - accuracy: 0.3321\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 1.4483 - accuracy: 0.3234\n",
      "*** Test loss: 1.448349989950657 - accuracy: 0.32344114780426025\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 4.2489\n",
      "Function value obtained: 1.4483\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "\t batch_size 86\n",
      "\t hidden_layer_size 183\n",
      "\t learning_rate 0.029720585788200724\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013e28f200> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013e28f200> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "140/140 [==============================] - 1s 7ms/step - loss: 1.4861 - accuracy: 0.2846\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4790 - accuracy: 0.3002\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4967 - accuracy: 0.2954\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4888 - accuracy: 0.2975\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4594 - accuracy: 0.2995\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4518 - accuracy: 0.3028\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4530 - accuracy: 0.2996\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4462 - accuracy: 0.3017\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4464 - accuracy: 0.3026\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 0s 2ms/step - loss: 1.4536 - accuracy: 0.3004\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 1.6983 - accuracy: 0.2904\n",
      "*** Test loss: 1.6983305862971714 - accuracy: 0.29043012857437134\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 4.1302\n",
      "Function value obtained: 1.6983\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "\t batch_size 26\n",
      "\t hidden_layer_size 242\n",
      "\t learning_rate 0.00019755775327416197\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f00f052bcb0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f00f052bcb0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "462/462 [==============================] - 2s 4ms/step - loss: 1.4824 - accuracy: 0.2936\n",
      "Epoch 2/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4454 - accuracy: 0.3071\n",
      "Epoch 3/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4405 - accuracy: 0.3160\n",
      "Epoch 4/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4368 - accuracy: 0.3209\n",
      "Epoch 5/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4336 - accuracy: 0.3240\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4306 - accuracy: 0.3277\n",
      "Epoch 7/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4280 - accuracy: 0.3301\n",
      "Epoch 8/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4256 - accuracy: 0.3299\n",
      "Epoch 9/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4234 - accuracy: 0.3305\n",
      "Epoch 10/10\n",
      "462/462 [==============================] - 1s 2ms/step - loss: 1.4214 - accuracy: 0.3315\n",
      "116/116 [==============================] - 0s 2ms/step - loss: 1.4461 - accuracy: 0.3144\n",
      "*** Test loss: 1.4460931040089706 - accuracy: 0.314438134431839\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 10.3112\n",
      "Function value obtained: 1.4461\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "\t batch_size 79\n",
      "\t hidden_layer_size 200\n",
      "\t learning_rate 0.0018931642317571234\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0118058170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f0118058170> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "152/152 [==============================] - 1s 7ms/step - loss: 1.4631 - accuracy: 0.2972\n",
      "Epoch 2/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4404 - accuracy: 0.3174\n",
      "Epoch 3/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4326 - accuracy: 0.3238\n",
      "Epoch 4/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4261 - accuracy: 0.3269\n",
      "Epoch 5/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4205 - accuracy: 0.3305\n",
      "Epoch 6/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4156 - accuracy: 0.3333\n",
      "Epoch 7/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4113 - accuracy: 0.3344\n",
      "Epoch 8/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4071 - accuracy: 0.3360\n",
      "Epoch 9/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.4034 - accuracy: 0.3408\n",
      "Epoch 10/10\n",
      "152/152 [==============================] - 0s 2ms/step - loss: 1.3997 - accuracy: 0.3412\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 1.4784 - accuracy: 0.3021\n",
      "*** Test loss: 1.4784441182487889 - accuracy: 0.30210068821907043\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 4.4904\n",
      "Function value obtained: 1.4784\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "\t batch_size 97\n",
      "\t hidden_layer_size 155\n",
      "\t learning_rate 0.005577986763085601\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013ddd69e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013ddd69e0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "124/124 [==============================] - 1s 7ms/step - loss: 1.4624 - accuracy: 0.2953\n",
      "Epoch 2/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4428 - accuracy: 0.3135\n",
      "Epoch 3/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4348 - accuracy: 0.3185\n",
      "Epoch 4/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4286 - accuracy: 0.3227\n",
      "Epoch 5/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4227 - accuracy: 0.3298\n",
      "Epoch 6/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4177 - accuracy: 0.3295\n",
      "Epoch 7/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4128 - accuracy: 0.3315\n",
      "Epoch 8/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4080 - accuracy: 0.3312\n",
      "Epoch 9/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4039 - accuracy: 0.3363\n",
      "Epoch 10/10\n",
      "124/124 [==============================] - 0s 2ms/step - loss: 1.4003 - accuracy: 0.3366\n",
      "31/31 [==============================] - 0s 6ms/step - loss: 1.4951 - accuracy: 0.3131\n",
      "*** Test loss: 1.4951381414167342 - accuracy: 0.3131043612957001\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 3.8554\n",
      "Function value obtained: 1.4951\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "\t batch_size 55\n",
      "\t hidden_layer_size 160\n",
      "\t learning_rate 0.005124835092681154\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013ddd6560> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f013ddd6560> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "219/219 [==============================] - 1s 5ms/step - loss: 1.4625 - accuracy: 0.2919\n",
      "Epoch 2/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4428 - accuracy: 0.3126\n",
      "Epoch 3/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4349 - accuracy: 0.3145\n",
      "Epoch 4/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4293 - accuracy: 0.3185\n",
      "Epoch 5/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4236 - accuracy: 0.3197\n",
      "Epoch 6/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4194 - accuracy: 0.3243\n",
      "Epoch 7/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4153 - accuracy: 0.3248\n",
      "Epoch 8/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4125 - accuracy: 0.3311\n",
      "Epoch 9/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4081 - accuracy: 0.3344\n",
      "Epoch 10/10\n",
      "219/219 [==============================] - 0s 2ms/step - loss: 1.4052 - accuracy: 0.3363\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 1.4912 - accuracy: 0.3124\n",
      "*** Test loss: 1.491245909170671 - accuracy: 0.3124374747276306\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 5.6499\n",
      "Function value obtained: 1.4912\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "\t batch_size 89\n",
      "\t hidden_layer_size 465\n",
      "\t learning_rate 0.07427781917044057\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f01183bc7a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f01183bc7a0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135/135 [==============================] - 1s 8ms/step - loss: 1.9747 - accuracy: 0.2741\n",
      "Epoch 2/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.2196 - accuracy: 0.2720\n",
      "Epoch 3/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.3898 - accuracy: 0.2724\n",
      "Epoch 4/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 2.0356 - accuracy: 0.2748\n",
      "Epoch 5/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 3.3196 - accuracy: 0.2744\n",
      "Epoch 6/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 4.8672 - accuracy: 0.2744\n",
      "Epoch 7/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7897 - accuracy: 0.2767\n",
      "Epoch 8/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.4872 - accuracy: 0.2746\n",
      "Epoch 9/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.7177 - accuracy: 0.2742\n",
      "Epoch 10/10\n",
      "135/135 [==============================] - 0s 3ms/step - loss: 1.5160 - accuracy: 0.2754\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 3.0022 - accuracy: 0.2578\n",
      "*** Test loss: 3.0021644515149735 - accuracy: 0.25775259733200073\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 4.5155\n",
      "Function value obtained: 3.0022\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "\t batch_size 36\n",
      "\t hidden_layer_size 132\n",
      "\t learning_rate 0.000632280200811115\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f01184de4d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f01184de4d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "334/334 [==============================] - 1s 4ms/step - loss: 1.4682 - accuracy: 0.2941\n",
      "Epoch 2/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4412 - accuracy: 0.3144\n",
      "Epoch 3/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4349 - accuracy: 0.3209\n",
      "Epoch 4/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4294 - accuracy: 0.3226\n",
      "Epoch 5/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4248 - accuracy: 0.3272\n",
      "Epoch 6/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4207 - accuracy: 0.3295\n",
      "Epoch 7/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4171 - accuracy: 0.3331\n",
      "Epoch 8/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4139 - accuracy: 0.3339\n",
      "Epoch 9/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4110 - accuracy: 0.3360\n",
      "Epoch 10/10\n",
      "334/334 [==============================] - 1s 2ms/step - loss: 1.4084 - accuracy: 0.3351\n",
      "84/84 [==============================] - 0s 5ms/step - loss: 1.4597 - accuracy: 0.3021\n",
      "*** Test loss: 1.4597455504394712 - accuracy: 0.30210068821907043\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 7.4869\n",
      "Function value obtained: 1.4597\n",
      "Current minimum: 1.4433\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "\t batch_size 18\n",
      "\t hidden_layer_size 380\n",
      "\t learning_rate 0.0014329798977768707\n",
      "Adding embedding of size 77 for layer Breed1\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f01340b7710> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f01340b7710> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
      "667/667 [==============================] - 2s 4ms/step - loss: 1.4582 - accuracy: 0.2969\n",
      "Epoch 2/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4422 - accuracy: 0.3106\n",
      "Epoch 3/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4339 - accuracy: 0.3192\n",
      "Epoch 4/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4262 - accuracy: 0.3253\n",
      "Epoch 5/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4195 - accuracy: 0.3271\n",
      "Epoch 6/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4129 - accuracy: 0.3351\n",
      "Epoch 7/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4075 - accuracy: 0.3371\n",
      "Epoch 8/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.4024 - accuracy: 0.3385\n",
      "Epoch 9/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.3979 - accuracy: 0.3384\n",
      "Epoch 10/10\n",
      "667/667 [==============================] - 1s 2ms/step - loss: 1.3938 - accuracy: 0.3408\n",
      "167/167 [==============================] - 0s 2ms/step - loss: 1.4957 - accuracy: 0.2964\n",
      "*** Test loss: 1.4957269879872213 - accuracy: 0.29643213748931885\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 15.0510\n",
      "Function value obtained: 1.4957\n",
      "Current minimum: 1.4433\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "exploration_result = gp_minimize(\n",
    "    objective_function, search_space_vals,\n",
    "    random_state=21, verbose=1, n_calls=iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value: 1.4433\n",
      "Best params:\n",
      "{'batch_size': 82, 'hidden_layer_size': 346, 'learning_rate': 0.00012836393729688734}\n"
     ]
    }
   ],
   "source": [
    "show_best(exploration_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deeplearning] *",
   "language": "python",
   "name": "conda-env-deeplearning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
