{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b02df97-cbb3-4759-9371-cbbecd0ccd86"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Construyendo una red convolucional con Keras\n",
    "\n",
    "El código de esta notebook, exceptuando el modelo en sí, es muy similar a la notebook 1 vista en la última clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "fa44eec5-93b3-4e4b-adcb-1065bb5cc474"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import optimizers, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "79849185-892a-41ce-b4a2-26db6ad19597"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cargando los datos del MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128  # For mini-batch gradient descent\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "input_size = 28*28\n",
    "train_examples = 60000\n",
    "test_examples = 10000\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the dataset to convert the examples from 2D matrixes to 1D arrays.\n",
    "x_train = x_train.reshape(train_examples, input_size)\n",
    "x_test = x_test.reshape(test_examples, input_size)\n",
    "\n",
    "# normalize the input\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos redimensionar el input para obtener la imagen en 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "img_rows, img_cols = 28, 28  # sqrt(784)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construímos nuestro modelo agregando una [capa convolucional](https://keras.io/layers/convolutional/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué tamaño tiene la salida de la primera convolución?\n",
    "\n",
    "(???, ???, ???, ???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5408)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               692352    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 693,962\n",
      "Trainable params: 693,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 2.2873 - accuracy: 0.1419 - val_loss: 0.5783 - val_accuracy: 0.2945\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 2.2444 - accuracy: 0.2163 - val_loss: 0.5676 - val_accuracy: 0.4797\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 2.1998 - accuracy: 0.2986 - val_loss: 0.5558 - val_accuracy: 0.5570\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 2.1507 - accuracy: 0.3737 - val_loss: 0.5429 - val_accuracy: 0.6004\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 2.0959 - accuracy: 0.4341 - val_loss: 0.5285 - val_accuracy: 0.6219\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 2.0360 - accuracy: 0.4803 - val_loss: 0.5123 - val_accuracy: 0.6422\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1.9691 - accuracy: 0.5170 - val_loss: 0.4947 - val_accuracy: 0.6582\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1.8992 - accuracy: 0.5448 - val_loss: 0.4761 - val_accuracy: 0.6766\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1.8246 - accuracy: 0.5693 - val_loss: 0.4568 - val_accuracy: 0.6887\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 1.7496 - accuracy: 0.5873 - val_loss: 0.4371 - val_accuracy: 0.7051\n",
      "Test loss: 1.6418530097961426\n",
      "Test accuracy: 0.7469\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test),\n",
    "          validation_steps=20)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN para texto\n",
    "\n",
    "A diferencia de las imágenes, el texto tiene, en principio, una sola dimensión por la cuál moverse, i.e. a través de la secuencia de palabras (eventualmente también de caracteres).\n",
    "\n",
    "Para ver como trabajar con CNNs en texto, una forma es utilizarlas como clasificadores de texto, en particular, podemos utilizar el conjunto de datos de IMDB sobre opiniones de películas para realizar tareas de análisis de sentimiento de las opiniones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Descargar los datos de IMDB\n",
    "mkdir -p dataset\n",
    "curl -L -o ./dataset/imdb_reviews.csv.gz https://cs.famaf.unc.edu.ar/~ccardellino/resources/diplodatos/imdb_reviews.csv.gz\n",
    "\n",
    "# Descargar los word embeddings (GloVe)\n",
    "curl -L -o ./dataset/glove.6B.zip https://cs.famaf.unc.edu.ar/~ccardellino/resources/diplodatos/glove.6B.zip\n",
    "unzip ./dataset/glove.6B.zip glove.6B.100d.txt -d ./dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/ccardellino/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ccardellino/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import SVG\n",
    "from gensim import corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download([\"punkt\", \"stopwords\"])\n",
    "\n",
    "sw = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del texto\n",
    "\n",
    "Para preparar este texto, haremos un preprocesamiento mínimo, tokenizaremos el texto, eliminaremos _stopwords_, llevaremos todo a minúsculas, y lo limitaremos a las 20000 palabras más frecuentes. \n",
    "\n",
    "Por otra parte, limitaremos las reviews a un máximo de 200 palabras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización\n",
    "\n",
    "Cargamos el conjunto de datos (reviews de IMDB), y lo tokenizamos mediante `word_tokenize` de NLTK. Además eliminamos las _stopwords_.\n",
    "\n",
    "A partir de estos tokens generaremos nuestro dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(\"./dataset/imdb_reviews.csv.gz\")\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_tokenization(review):\n",
    "    return [word.lower() for word in word_tokenize(review, language=\"english\") if word.lower() not in sw]\n",
    "\n",
    "review_tokens = reviews[\"review\"].apply(review_tokenization).values.tolist()\n",
    "target = reviews[\"sentiment\"].map({\"positive\": 1, \"negative\": 0}).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulario\n",
    "\n",
    "Con ayuda de `gensim` creamos el diccionario del vocabulario (mapeando palabras a índices). Al mismo tiempo, limitamos el tamaño del mismo a las 20000 palabras más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = corpora.Dictionary(review_tokens)\n",
    "vocabulary.filter_extremes(no_below=1, no_above=1.0, keep_n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos\n",
    "\n",
    "### Dataset de TF\n",
    "\n",
    "Ya con el vocabulario generado, podemos crear nuestro `tf.data.Dataset` para poder entrenar nuestros modelos.\n",
    "\n",
    "Para ello tenemos que generar los ejemplos a trabajar, convirtiendo nuestros tokens a índices. Aquellos tokens que no estén en el vocabulario serán reemplazados por un token de \"palabra desconocida\" cuyo índice será igual al tamaño del vocabulario (recordemos que el vocabulario arranca desde el índice 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=16862, shape=(237,), dtype=int64, numpy=\n",
      "array([   91,   104,    83,   137,     8,    94,    41,     0,    67,\n",
      "           4,   105,     3,    43,    62,    82,     9,    20,     7,\n",
      "          10,     9,    20,     7,    10,    50,   126,   123,    94,\n",
      "          21,   132,   109,   135,     3,   112,   105,   139,    57,\n",
      "           4,   129,     3,   115,    48,    64,   127,     4,   115,\n",
      "         100,   101,   103,    38,     3,   113,   135,     4,    63,\n",
      "           3,    28,   133, 20000,     9,    20,     7,    10,     9,\n",
      "          20,     7,    10,    22,    94,    90,    55,    93,    80,\n",
      "         111,   121, 20000,     4,    51,    77, 20000,    26,     3,\n",
      "          45,   110,    98,    23,    56,    53,    46, 20000,     3,\n",
      "          99,    65,    12,     4,    40,    26,    66, 20000,     3,\n",
      "          86,     3, 20000,     3,    74,     3,    25,     3,    71,\n",
      "           3,    70,     5,     6, 20000,     3,    35,   120,     3,\n",
      "          37,    34,   114, 20000,    89,    49,    17,     9,    20,\n",
      "           7,    10,     9,    20,     7,    10,   140,   108,    76,\n",
      "          13,   115,    39,    47,    58,   116,   140,    87,    32,\n",
      "           4,    52,    97,    96,    95,    78,    15,     3,    52,\n",
      "          24,     3,    52,   106,     5,    94,    87,    84,    14,\n",
      "           4,    50,    41,    42,   107,   123,    88,   124,     3,\n",
      "          30,    87,   108,   102,     3,   136,     3,    36,   125,\n",
      "          94,     3,    59,    11,    65,    75,    60,   135,     4,\n",
      "         135,     3,    68,     1,    31,    61,     0,   119, 20000,\n",
      "           3,    69,     0,    72,    92,    54,    16,     3,   138,\n",
      "          79,     3,    85,    27,    69,   130,    98,    19,    39,\n",
      "          73,   122,   118,    98,    44,     2,   137,    94,     3,\n",
      "          81,    18,    29,   131,   134,     5, 20000,    54,   128,\n",
      "          33,   117,     4])>, <tf.Tensor: id=16863, shape=(), dtype=int64, numpy=1>)\n",
      "(<tf.Tensor: id=16864, shape=(129,), dtype=int64, numpy=\n",
      "array([  202,   170,   179,     4,     9,    20,     7,    10,     9,\n",
      "          20,     7,    10,   162,   194, 20000, 20000,   161,   164,\n",
      "         150,     3,   192, 20000,     3,   187,   181,   156,   177,\n",
      "           4,     9,    20,     7,    10,     9,    20,     7,    10,\n",
      "         147,   159,   138, 20000,   173,   190,   146,    59, 20000,\n",
      "         143,   200,   175,   141,   199,   186,   185,   155,   167,\n",
      "         183,   201,   142,   152,   157,     3,   138,   203,   137,\n",
      "       20000,   204,   176,   177,     4,   172,   179,    91,   165,\n",
      "         171,   144,   148,   169,     4,     9,    20,     7,    10,\n",
      "           9,    20,     7,    10,   181,   182,   149,    66,   170,\n",
      "         197,   145,   160,   166,     3,   180,   133,   198, 20000,\n",
      "         142,   195,   184,   191,   153,     4,   178,   168,   188,\n",
      "           3,   174,   109,   151, 20000, 20000,   189,     1,   174,\n",
      "         163, 20000,   144, 20000, 20000,   158,   193,     2,   196,\n",
      "         138,   154,     4])>, <tf.Tensor: id=16865, shape=(), dtype=int64, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "def dataset_generator():\n",
    "    for rev_tok, lbl in zip(review_tokens, target):\n",
    "        rev_tok_seq = vocabulary.doc2idx(rev_tok, unknown_word_index=len(vocabulary))\n",
    "        yield (rev_tok_seq, lbl)\n",
    "\n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    dataset_generator, \n",
    "    output_types=(tf.int64, tf.int64)\n",
    ")\n",
    "\n",
    "# Ejemplo de como se ve nuestro dataset\n",
    "for ds in dataset.take(2):\n",
    "    print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding de secuencias\n",
    "\n",
    "Una vez creado nuestro dataset podemos dividirlo en los conjuntos de entrenamiento y evaluación. \n",
    "\n",
    "Además, tenemos que hacer _padding_ de las secuencias para normalizar el tamaño a un valor fijo (200 palabras). Para eso podemos utilizar lo que ofrece Keras. En particular, como los índices del `0` a `len(vocabulary)` están \"ocupados\", haremos el _padding_ con un nuevo valor igual a `len(vocabulary) + 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(reviews.shape[0] * 0.8)\n",
    "TEST_SIZE = reviews.shape[0] - TRAIN_SIZE\n",
    "BATCH_SIZE = 128\n",
    "MAX_SEQUENCE_LEN = 200\n",
    "\n",
    "# First, truncate all the data to a maximum of MAX_SEQUENCE_LEN words\n",
    "dataset = dataset.map(lambda data, target: (data[:MAX_SEQUENCE_LEN], target))\n",
    "\n",
    "train_data = dataset.skip(TEST_SIZE).shuffle(TRAIN_SIZE)\n",
    "test_data = dataset.take(TEST_SIZE)\n",
    "\n",
    "# Pad the datasets\n",
    "padding_shapes = ([MAX_SEQUENCE_LEN], [])  # Pad sequence up to maximum value (in this case MAX_SEQUENCE_LEN)\n",
    "padding_values = (np.int64(len(vocabulary) + 1), np.int64(0))\n",
    "\n",
    "train_data = train_data.padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)\n",
    "test_data = test_data.padded_batch(BATCH_SIZE, padded_shapes=padding_shapes, padding_values=padding_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del modelo\n",
    "\n",
    "### Preparación de la matriz de embeddings\n",
    "\n",
    "Si bien los embeddings pueden sen calculados en tiempo de entrenamiento, en general los embeddings pre-entrenados suelen tener mejores resultados y además quitan parámetros a ajustar durante el entrenamiento de la red.\n",
    "\n",
    "Podemos cargar los embeddings de GloVe desde los archivos de texto que los definen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19137 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(\"./dataset/glove.6B.100d.txt\", \"r\") as fh:\n",
    "    for line in fh:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in vocabulary.token2id:  # Only use the embeddings of words in our vocabulary\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, no todas las palabras de nuestro vocabulario tienen un vector asociado, para estos casos crearemos un vector al azar. \n",
    "\n",
    "Además, necesitaremos crear un vector al azar para el caso de las palabras desconocidas, asociadas al índice `len(vocabulary)` y un vector de ceros asociado al índice `len(vocabulary)+1`. Por lo que nuestra matriz de embeddings tendrá un total de `len(vocabulary)+2` embeddings.\n",
    "\n",
    "A partir de esto podemos crear nuestra matriz de embeddings con la que inicializaremos nuestra capa de embeddings en nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_DIM = 100  # Given by the model (in this case glove.6B.100d)\n",
    "\n",
    "embedding_matrix = np.zeros((len(vocabulary) + 2, 100))\n",
    "\n",
    "for widx, word in vocabulary.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[widx] = embedding_vector\n",
    "    else:\n",
    "        # Random normal initialization for words without embeddings\n",
    "        embedding_matrix[widx] = np.random.normal(size=(100,))  \n",
    "\n",
    "# Random normal initialization for unknown words\n",
    "embedding_matrix[len(vocabulary)] = np.random.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creación del modelo\n",
    "\n",
    "Una vez que tenemos todos los datos necesarios, el paso final es la creación y entrenamiento del modelo utilizando las herramientas de TF y Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"TextCNN\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "word_embedding (Embedding)      (None, 200, 100)     2000200     input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_words (Conv1D)           (None, 199, 64)      12864       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_3_words (Conv1D)           (None, 198, 64)      19264       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv_5_words (Conv1D)           (None, 196, 64)      32064       word_embedding[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_2_words (GlobalMaxPool (None, 64)           0           conv_2_words[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_3_words (GlobalMaxPool (None, 64)           0           conv_3_words[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_5_words (GlobalMaxPool (None, 64)           0           conv_5_words[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "convolved_features (Concatenate (None, 192)          0           max_pool_2_words[0][0]           \n",
      "                                                                 max_pool_3_words[0][0]           \n",
      "                                                                 max_pool_5_words[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            193         convolved_features[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 2,064,585\n",
      "Trainable params: 64,385\n",
      "Non-trainable params: 2,000,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "FILTER_WIDTHS = [2, 3, 5]  # Take 2, 3, and 5 words\n",
    "FILTER_COUNT = 64\n",
    "\n",
    "sequence_input = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int64', name=\"input\")\n",
    "\n",
    "# The embedding layer is initialized with our embedding_matrix and is not trainable\n",
    "embeddings_layer = tf.keras.layers.Embedding(\n",
    "    embedding_matrix.shape[0],\n",
    "    EMBEDDINGS_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LEN,\n",
    "    trainable=False,\n",
    "    name=\"word_embedding\"\n",
    ")\n",
    "\n",
    "embedded_sequences = embeddings_layer(sequence_input)\n",
    "\n",
    "conv_layers = []\n",
    "for filter_width in FILTER_WIDTHS:\n",
    "    layer = tf.keras.layers.Conv1D(\n",
    "        FILTER_COUNT,\n",
    "        filter_width,\n",
    "        activation=\"relu\",\n",
    "        name=\"conv_{}_words\".format(filter_width)\n",
    "    )(embedded_sequences)\n",
    "    layer = tf.keras.layers.GlobalMaxPooling1D(name=\"max_pool_{}_words\".format(filter_width))(layer)\n",
    "    conv_layers.append(layer)\n",
    "\n",
    "convolved_features = tf.keras.layers.Concatenate(name=\"convolved_features\")(conv_layers)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"output\")(convolved_features)\n",
    "model = tf.keras.models.Model(inputs=[sequence_input], outputs=[output], name=\"TextCNN\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar un grafo del modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"364pt\" viewBox=\"0.00 0.00 803.00 410.00\" width=\"714pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(.8889 .8889) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 799,-406 799,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140539775282928 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140539775282928</title>\n",
       "<polygon fill=\"none\" points=\"341.5,-365.5 341.5,-401.5 453.5,-401.5 453.5,-365.5 341.5,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-379.8\">input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140539775282872 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140539775282872</title>\n",
       "<polygon fill=\"none\" points=\"306.5,-292.5 306.5,-328.5 488.5,-328.5 488.5,-292.5 306.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-306.8\">word_embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140539775282928&#45;&gt;140539775282872 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140539775282928-&gt;140539775282872</title>\n",
       "<path d=\"M397.5,-365.4551C397.5,-357.3828 397.5,-347.6764 397.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-338.5903 397.5,-328.5904 394.0001,-338.5904 401.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539783278152 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140539783278152</title>\n",
       "<polygon fill=\"none\" points=\"104,-219.5 104,-255.5 253,-255.5 253,-219.5 104,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"178.5\" y=\"-233.8\">conv_2_words: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140539775282872&#45;&gt;140539783278152 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140539775282872-&gt;140539783278152</title>\n",
       "<path d=\"M343.3652,-292.4551C312.8311,-282.277 274.5028,-269.5009 242.4883,-258.8294\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"243.3647,-255.4323 232.7711,-255.5904 241.1511,-262.0731 243.3647,-255.4323\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539775327872 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140539775327872</title>\n",
       "<polygon fill=\"none\" points=\"323,-219.5 323,-255.5 472,-255.5 472,-219.5 323,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-233.8\">conv_3_words: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140539775282872&#45;&gt;140539775327872 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140539775282872-&gt;140539775327872</title>\n",
       "<path d=\"M397.5,-292.4551C397.5,-284.3828 397.5,-274.6764 397.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-265.5903 397.5,-255.5904 394.0001,-265.5904 401.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539774962488 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140539774962488</title>\n",
       "<polygon fill=\"none\" points=\"542,-219.5 542,-255.5 691,-255.5 691,-219.5 542,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"616.5\" y=\"-233.8\">conv_5_words: Conv1D</text>\n",
       "</g>\n",
       "<!-- 140539775282872&#45;&gt;140539774962488 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140539775282872-&gt;140539774962488</title>\n",
       "<path d=\"M451.6348,-292.4551C482.1689,-282.277 520.4972,-269.5009 552.5117,-258.8294\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"553.8489,-262.0731 562.2289,-255.5904 551.6353,-255.4323 553.8489,-262.0731\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539775325240 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140539775325240</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 253,-182.5 253,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"126.5\" y=\"-160.8\">max_pool_2_words: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140539783278152&#45;&gt;140539775325240 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140539783278152-&gt;140539775325240</title>\n",
       "<path d=\"M165.6461,-219.4551C159.521,-210.8564 152.075,-200.4034 145.328,-190.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"148.0389,-188.7046 139.3863,-182.5904 142.3374,-192.7659 148.0389,-188.7046\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539775377304 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140539775377304</title>\n",
       "<polygon fill=\"none\" points=\"271,-146.5 271,-182.5 524,-182.5 524,-146.5 271,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-160.8\">max_pool_3_words: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140539775327872&#45;&gt;140539775377304 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140539775327872-&gt;140539775377304</title>\n",
       "<path d=\"M397.5,-219.4551C397.5,-211.3828 397.5,-201.6764 397.5,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-192.5903 397.5,-182.5904 394.0001,-192.5904 401.0001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539775008160 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140539775008160</title>\n",
       "<polygon fill=\"none\" points=\"542,-146.5 542,-182.5 795,-182.5 795,-146.5 542,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"668.5\" y=\"-160.8\">max_pool_5_words: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 140539774962488&#45;&gt;140539775008160 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140539774962488-&gt;140539775008160</title>\n",
       "<path d=\"M629.3539,-219.4551C635.479,-210.8564 642.925,-200.4034 649.672,-190.9316\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"652.6626,-192.7659 655.6137,-182.5904 646.9611,-188.7046 652.6626,-192.7659\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140541489354456 -->\n",
       "<g class=\"node\" id=\"node9\">\n",
       "<title>140541489354456</title>\n",
       "<polygon fill=\"none\" points=\"299,-73.5 299,-109.5 496,-109.5 496,-73.5 299,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-87.8\">convolved_features: Concatenate</text>\n",
       "</g>\n",
       "<!-- 140539775325240&#45;&gt;140541489354456 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>140539775325240-&gt;140541489354456</title>\n",
       "<path d=\"M193.4888,-146.4551C232.0058,-136.0796 280.5456,-123.0043 320.6145,-112.2109\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"321.5972,-115.571 330.3426,-109.5904 319.7764,-108.8119 321.5972,-115.571\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539775377304&#45;&gt;140541489354456 -->\n",
       "<g class=\"edge\" id=\"edge9\">\n",
       "<title>140539775377304-&gt;140541489354456</title>\n",
       "<path d=\"M397.5,-146.4551C397.5,-138.3828 397.5,-128.6764 397.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-119.5903 397.5,-109.5904 394.0001,-119.5904 401.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140539775008160&#45;&gt;140541489354456 -->\n",
       "<g class=\"edge\" id=\"edge10\">\n",
       "<title>140539775008160-&gt;140541489354456</title>\n",
       "<path d=\"M601.5112,-146.4551C562.9942,-136.0796 514.4544,-123.0043 474.3855,-112.2109\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"475.2236,-108.8119 464.6574,-109.5904 473.4028,-115.571 475.2236,-108.8119\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140541489353728 -->\n",
       "<g class=\"node\" id=\"node10\">\n",
       "<title>140541489353728</title>\n",
       "<polygon fill=\"none\" points=\"351.5,-.5 351.5,-36.5 443.5,-36.5 443.5,-.5 351.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-14.8\">output: Dense</text>\n",
       "</g>\n",
       "<!-- 140541489354456&#45;&gt;140541489353728 -->\n",
       "<g class=\"edge\" id=\"edge11\">\n",
       "<title>140541489354456-&gt;140541489353728</title>\n",
       "<path d=\"M397.5,-73.4551C397.5,-65.3828 397.5,-55.6764 397.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"401.0001,-46.5903 397.5,-36.5904 394.0001,-46.5904 401.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(tf.keras.utils.model_to_dot(model, dpi=64).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.4959 - accuracy: 0.7538 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.3497 - accuracy: 0.8482 - val_loss: 0.3543 - val_accuracy: 0.8387\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.3002 - accuracy: 0.8744 - val_loss: 0.3372 - val_accuracy: 0.8469\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 22s 71ms/step - loss: 0.2670 - accuracy: 0.8927 - val_loss: 0.3156 - val_accuracy: 0.8605\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.2398 - accuracy: 0.9075 - val_loss: 0.3070 - val_accuracy: 0.8672\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 0.2087 - accuracy: 0.9211 - val_loss: 0.3040 - val_accuracy: 0.8680\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.1864 - accuracy: 0.9329 - val_loss: 0.3254 - val_accuracy: 0.8543\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 24s 77ms/step - loss: 0.1613 - accuracy: 0.9467 - val_loss: 0.3010 - val_accuracy: 0.8715\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 23s 72ms/step - loss: 0.1384 - accuracy: 0.9581 - val_loss: 0.3062 - val_accuracy: 0.8707\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 22s 72ms/step - loss: 0.1220 - accuracy: 0.9648 - val_loss: 0.3216 - val_accuracy: 0.8641\n",
      "Test loss: 0.317\n",
      "Test accuracy: 0.870\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_data,\n",
    "    epochs=10,\n",
    "    validation_data=test_data,\n",
    "    validation_steps=20\n",
    ")\n",
    "\n",
    "score = model.evaluate(test_data, verbose=0)\n",
    "print('Test loss: {:.03f}'.format(score[0]))\n",
    "print('Test accuracy: {:.03f}'.format(score[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
